{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb963b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:36.570422Z",
     "start_time": "2025-02-15T17:56:31.465373Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 02:56:33.243273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7674eaf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:37.862448Z",
     "start_time": "2025-02-15T17:56:36.572256Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e28ca75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:37.869495Z",
     "start_time": "2025-02-15T17:56:37.865698Z"
    }
   },
   "outputs": [],
   "source": [
    "# % Used Smartphone: LGE-lge-LG-H815-5.1                                             \n",
    "# % Smartphone's Accelerometer: LGE Accelerometer - Vendor: BOSCH                    \n",
    "# % --> Version: 1                                                                   \n",
    "# % --> Min - Max Delay: 5000us - 65535000us                                         \n",
    "# % --> Maximum Range: 16.000000263891405 G                                          \n",
    "# % --> Resolution: 1.2136514986004396E-4 G                                          \n",
    "                                                                                    \n",
    "# % SensorTag's Accelerometer: MPU-9250 MEMS MotionTracking Device - Invensense      \n",
    "# % --> Maximum Range: 16 G                                                          \n",
    "# % --> Resolution: 0.00024 G                                                        \n",
    "                                                                                    \n",
    "# % MAC Address; Sensor_ID; Position; Device Model                                   \n",
    "# %f8:95:c7:f3:ba:82; 0; RIGHTPOCKET; lge-LG-H815-5.1                                \n",
    "# %C4:BE:84:70:64:8A; 1; CHEST; SensorTag                                            \n",
    "# %C4:BE:84:70:0E:80; 3; WRIST; SensorTag                                            \n",
    "# %C4:BE:84:71:A5:02; 2; WAIST; SensorTag                                            \n",
    "# %B0:B4:48:B8:77:03; 4; ANKLE; SensorTag                                            \n",
    "                                                                                    \n",
    "# % Sensor_Type:                                                                     \n",
    "# % Accelerometer = 0                                                                \n",
    "# % Gyroscope = 1                                                                    \n",
    "# % Magnetometer = 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a22e79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:37.875942Z",
     "start_time": "2025-02-15T17:56:37.870897Z"
    }
   },
   "outputs": [],
   "source": [
    "datapath = \"../CAGE/data/UMAFall_Dataset/\"\n",
    "all_files = glob.glob(os.path.join(datapath, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20da022",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:37.880591Z",
     "start_time": "2025-02-15T17:56:37.877123Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    pattern = r'UMAFall_Subject_(\\d+)_(ADL|Fall)_([a-zA-Z_]+)_(\\d+)_(\\d{4}-\\d{2}-\\d{2})_.*\\.csv'\n",
    "    match = re.match(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        subject_id = int(match.group(1))\n",
    "        activity_type = match.group(2)\n",
    "        activity = match.group(3)\n",
    "        trial = int(match.group(4))\n",
    "        date = match.group(5)\n",
    "        return subject_id, activity_type, activity, trial, date\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3087517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:37.890701Z",
     "start_time": "2025-02-15T17:56:37.883231Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_all_sensors_file(filepath):\n",
    "    sensor_data = {\n",
    "        'RIGHTPOCKET': {'id': None, 'data': []},\n",
    "        'CHEST': {'id': None, 'data': []},\n",
    "        'WRIST': {'id': None, 'data': []},\n",
    "        'WAIST': {'id': None, 'data': []},\n",
    "        'ANKLE': {'id': None, 'data': []}\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if ';' in line and any(pos in line for pos in sensor_data.keys()):\n",
    "                parts = line.strip().split(';')\n",
    "                if len(parts) >= 3:\n",
    "                    sensor_id = parts[1].strip()\n",
    "                    position = parts[2].strip()\n",
    "                    if position in sensor_data:\n",
    "                        sensor_data[position]['id'] = sensor_id\n",
    "        \n",
    "        data_start = False\n",
    "        for line in lines:\n",
    "            if '% TimeStamp; Sample No;' in line:\n",
    "                data_start = True\n",
    "                continue\n",
    "            \n",
    "            if data_start:\n",
    "                try:\n",
    "                    values = [float(v.strip()) for v in line.split(';')]\n",
    "                    sensor_id = str(int(values[-1]))\n",
    "                    sensor_type = int(values[-2])\n",
    "                    data = values[2:5]\n",
    "                    \n",
    "                    for position, info in sensor_data.items():\n",
    "                        if info['id'] == sensor_id:\n",
    "                            info['data'].append([*data, sensor_type])\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    for position in sensor_data:\n",
    "        if sensor_data[position]['data']:\n",
    "            sensor_data[position]['data'] = np.array(sensor_data[position]['data'])\n",
    "        else:\n",
    "            sensor_data[position]['data'] = None\n",
    "            \n",
    "    return sensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22cc6d3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:37.901881Z",
     "start_time": "2025-02-15T17:56:37.892586Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_sensor_data(sensor_data):\n",
    "    processed_data = {}\n",
    "    valid_lengths = []\n",
    "    \n",
    "    for position, data in sensor_data.items():\n",
    "        if data['data'] is not None:\n",
    "            acc_mask = data['data'][:, -1] == 0\n",
    "            gyro_mask = data['data'][:, -1] == 1\n",
    "            mag_mask = data['data'][:, -1] == 2\n",
    "            \n",
    "            acc_data = data['data'][acc_mask][:, :3] if np.any(acc_mask) else np.array([])\n",
    "            gyro_data = data['data'][gyro_mask][:, :3] if np.any(gyro_mask) else np.array([])\n",
    "            mag_data = data['data'][mag_mask][:, :3] if np.any(mag_mask) else np.array([])\n",
    "            \n",
    "            if len(acc_data) > 0:\n",
    "                valid_lengths.append(len(acc_data))\n",
    "            if len(gyro_data) > 0:\n",
    "                valid_lengths.append(len(gyro_data))\n",
    "            if len(mag_data) > 0:\n",
    "                valid_lengths.append(len(mag_data))\n",
    "    \n",
    "    if not valid_lengths:\n",
    "        return None\n",
    "        \n",
    "    min_length = min(valid_lengths)\n",
    "    \n",
    "    for position, data in sensor_data.items():\n",
    "        if data['data'] is not None:\n",
    "            acc_mask = data['data'][:, -1] == 0\n",
    "            gyro_mask = data['data'][:, -1] == 1\n",
    "            mag_mask = data['data'][:, -1] == 2\n",
    "            \n",
    "            if np.any(acc_mask):\n",
    "                acc_data = data['data'][acc_mask][:min_length, :3]\n",
    "                acc_cols = [f'{position.lower()}_acc_x', f'{position.lower()}_acc_y', f'{position.lower()}_acc_z']\n",
    "                processed_data.update(dict(zip(acc_cols, acc_data.T)))\n",
    "            \n",
    "            if np.any(gyro_mask):\n",
    "                gyro_data = data['data'][gyro_mask][:min_length, :3]\n",
    "                gyro_cols = [f'{position.lower()}_gyro_x', f'{position.lower()}_gyro_y', f'{position.lower()}_gyro_z']\n",
    "                processed_data.update(dict(zip(gyro_cols, gyro_data.T)))\n",
    "            \n",
    "            if np.any(mag_mask):\n",
    "                mag_data = data['data'][mag_mask][:min_length, :3]\n",
    "                mag_cols = [f'{position.lower()}_mag_x', f'{position.lower()}_mag_y', f'{position.lower()}_mag_z']\n",
    "                processed_data.update(dict(zip(mag_cols, mag_data.T)))\n",
    "    \n",
    "    if not processed_data:\n",
    "        return None\n",
    "        \n",
    "    return pd.DataFrame(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3dd5d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:37.906111Z",
     "start_time": "2025-02-15T17:56:37.903772Z"
    }
   },
   "outputs": [],
   "source": [
    "subject_id = 18\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a1e659",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.082155Z",
     "start_time": "2025-02-15T17:56:37.911698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 746/746 [00:05<00:00, 145.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(all_files):\n",
    "    filename = os.path.basename(file)\n",
    "    info = parse_filename(filename)\n",
    "    \n",
    "    if info and info[0] == subject_id:\n",
    "        sensor_data = read_all_sensors_file(file)\n",
    "        df = process_sensor_data(sensor_data)\n",
    "        \n",
    "        if df is not None:\n",
    "            df['subject'] = info[0]\n",
    "            df['activity'] = info[2]  \n",
    "            df['trial'] = info[3]\n",
    "            dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805a35d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.174034Z",
     "start_time": "2025-02-15T17:56:43.084716Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape :  (56241, 42)\n",
      "columns : ['rightpocket_acc_x', 'rightpocket_acc_y', 'rightpocket_acc_z', 'chest_acc_x', 'chest_acc_y', 'chest_acc_z', 'chest_gyro_x', 'chest_gyro_y', 'chest_gyro_z', 'chest_mag_x', 'chest_mag_y', 'chest_mag_z', 'wrist_acc_x', 'wrist_acc_y', 'wrist_acc_z', 'wrist_gyro_x', 'wrist_gyro_y', 'wrist_gyro_z', 'wrist_mag_x', 'wrist_mag_y', 'wrist_mag_z', 'waist_acc_x', 'waist_acc_y', 'waist_acc_z', 'waist_gyro_x', 'waist_gyro_y', 'waist_gyro_z', 'waist_mag_x', 'waist_mag_y', 'waist_mag_z', 'ankle_acc_x', 'ankle_acc_y', 'ankle_acc_z', 'ankle_gyro_x', 'ankle_gyro_y', 'ankle_gyro_z', 'ankle_mag_x', 'ankle_mag_y', 'ankle_mag_z']\n",
      "\n",
      "Columns after concat: Index(['rightpocket_acc_x', 'rightpocket_acc_y', 'rightpocket_acc_z',\n",
      "       'chest_acc_x', 'chest_acc_y', 'chest_acc_z', 'chest_gyro_x',\n",
      "       'chest_gyro_y', 'chest_gyro_z', 'chest_mag_x', 'chest_mag_y',\n",
      "       'chest_mag_z', 'wrist_acc_x', 'wrist_acc_y', 'wrist_acc_z',\n",
      "       'wrist_gyro_x', 'wrist_gyro_y', 'wrist_gyro_z', 'wrist_mag_x',\n",
      "       'wrist_mag_y', 'wrist_mag_z', 'waist_acc_x', 'waist_acc_y',\n",
      "       'waist_acc_z', 'waist_gyro_x', 'waist_gyro_y', 'waist_gyro_z',\n",
      "       'waist_mag_x', 'waist_mag_y', 'waist_mag_z', 'ankle_acc_x',\n",
      "       'ankle_acc_y', 'ankle_acc_z', 'ankle_gyro_x', 'ankle_gyro_y',\n",
      "       'ankle_gyro_z', 'ankle_mag_x', 'ankle_mag_y', 'ankle_mag_z', 'subject',\n",
      "       'activity', 'trial'],\n",
      "      dtype='object')\n",
      "\n",
      "Activity distribution : activity\n",
      "forwardFall                  5925\n",
      "backwardFall                 5668\n",
      "lateralFall                  5081\n",
      "Sitting_GettingUpOnAChair    4485\n",
      "Bending                      4465\n",
      "LyingDown_OnABed             4464\n",
      "Walking                      4186\n",
      "Hopping                      4167\n",
      "MakingACall                  3590\n",
      "Aplausing                    2693\n",
      "OpeningDoor                  2692\n",
      "HandsUp                      2689\n",
      "Jogging                      2329\n",
      "GoUpstairs                   2046\n",
      "GoDownstairs                 1761\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features = pd.concat(dfs, ignore_index=True)\n",
    "print (\"shape : \", features.shape)\n",
    "sensor_columns = [col for col in features.columns if any(x in col for x in ['acc', 'gyro', 'mag'])]\n",
    "print (\"columns :\", end=' ')\n",
    "print (sensor_columns)\n",
    "\n",
    "print ()\n",
    "features = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Columns after concat:\", features.columns)\n",
    "print(\"\\nActivity distribution :\", features['activity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518520e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.200718Z",
     "start_time": "2025-02-15T17:56:43.176150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activity distribution : activity\n",
      "0    39567\n",
      "1    16674\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features['activity'] = features['activity'].map(\n",
    "    lambda x: 1 if x in ['forwardFall', 'backwardFall', 'lateralFall'] else 0\n",
    ")\n",
    "print(\"\\nActivity distribution :\", features['activity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c675928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.219043Z",
     "start_time": "2025-02-15T17:56:43.203091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56241 entries, 0 to 56240\n",
      "Data columns (total 42 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   rightpocket_acc_x  56241 non-null  float64\n",
      " 1   rightpocket_acc_y  56241 non-null  float64\n",
      " 2   rightpocket_acc_z  56241 non-null  float64\n",
      " 3   chest_acc_x        56241 non-null  float64\n",
      " 4   chest_acc_y        56241 non-null  float64\n",
      " 5   chest_acc_z        56241 non-null  float64\n",
      " 6   chest_gyro_x       56241 non-null  float64\n",
      " 7   chest_gyro_y       56241 non-null  float64\n",
      " 8   chest_gyro_z       56241 non-null  float64\n",
      " 9   chest_mag_x        56241 non-null  float64\n",
      " 10  chest_mag_y        56241 non-null  float64\n",
      " 11  chest_mag_z        56241 non-null  float64\n",
      " 12  wrist_acc_x        56241 non-null  float64\n",
      " 13  wrist_acc_y        56241 non-null  float64\n",
      " 14  wrist_acc_z        56241 non-null  float64\n",
      " 15  wrist_gyro_x       56241 non-null  float64\n",
      " 16  wrist_gyro_y       56241 non-null  float64\n",
      " 17  wrist_gyro_z       56241 non-null  float64\n",
      " 18  wrist_mag_x        56241 non-null  float64\n",
      " 19  wrist_mag_y        56241 non-null  float64\n",
      " 20  wrist_mag_z        56241 non-null  float64\n",
      " 21  waist_acc_x        52434 non-null  float64\n",
      " 22  waist_acc_y        52434 non-null  float64\n",
      " 23  waist_acc_z        52434 non-null  float64\n",
      " 24  waist_gyro_x       52434 non-null  float64\n",
      " 25  waist_gyro_y       52434 non-null  float64\n",
      " 26  waist_gyro_z       52434 non-null  float64\n",
      " 27  waist_mag_x        52434 non-null  float64\n",
      " 28  waist_mag_y        52434 non-null  float64\n",
      " 29  waist_mag_z        52434 non-null  float64\n",
      " 30  ankle_acc_x        26210 non-null  float64\n",
      " 31  ankle_acc_y        26210 non-null  float64\n",
      " 32  ankle_acc_z        26210 non-null  float64\n",
      " 33  ankle_gyro_x       26210 non-null  float64\n",
      " 34  ankle_gyro_y       26210 non-null  float64\n",
      " 35  ankle_gyro_z       26210 non-null  float64\n",
      " 36  ankle_mag_x        26210 non-null  float64\n",
      " 37  ankle_mag_y        26210 non-null  float64\n",
      " 38  ankle_mag_z        26210 non-null  float64\n",
      " 39  subject            56241 non-null  int64  \n",
      " 40  activity           56241 non-null  int64  \n",
      " 41  trial              56241 non-null  int64  \n",
      "dtypes: float64(39), int64(3)\n",
      "memory usage: 18.0 MB\n"
     ]
    }
   ],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31b85551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.243432Z",
     "start_time": "2025-02-15T17:56:43.220801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22403 entries, 0 to 55944\n",
      "Data columns (total 42 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   rightpocket_acc_x  22403 non-null  float64\n",
      " 1   rightpocket_acc_y  22403 non-null  float64\n",
      " 2   rightpocket_acc_z  22403 non-null  float64\n",
      " 3   chest_acc_x        22403 non-null  float64\n",
      " 4   chest_acc_y        22403 non-null  float64\n",
      " 5   chest_acc_z        22403 non-null  float64\n",
      " 6   chest_gyro_x       22403 non-null  float64\n",
      " 7   chest_gyro_y       22403 non-null  float64\n",
      " 8   chest_gyro_z       22403 non-null  float64\n",
      " 9   chest_mag_x        22403 non-null  float64\n",
      " 10  chest_mag_y        22403 non-null  float64\n",
      " 11  chest_mag_z        22403 non-null  float64\n",
      " 12  wrist_acc_x        22403 non-null  float64\n",
      " 13  wrist_acc_y        22403 non-null  float64\n",
      " 14  wrist_acc_z        22403 non-null  float64\n",
      " 15  wrist_gyro_x       22403 non-null  float64\n",
      " 16  wrist_gyro_y       22403 non-null  float64\n",
      " 17  wrist_gyro_z       22403 non-null  float64\n",
      " 18  wrist_mag_x        22403 non-null  float64\n",
      " 19  wrist_mag_y        22403 non-null  float64\n",
      " 20  wrist_mag_z        22403 non-null  float64\n",
      " 21  waist_acc_x        22403 non-null  float64\n",
      " 22  waist_acc_y        22403 non-null  float64\n",
      " 23  waist_acc_z        22403 non-null  float64\n",
      " 24  waist_gyro_x       22403 non-null  float64\n",
      " 25  waist_gyro_y       22403 non-null  float64\n",
      " 26  waist_gyro_z       22403 non-null  float64\n",
      " 27  waist_mag_x        22403 non-null  float64\n",
      " 28  waist_mag_y        22403 non-null  float64\n",
      " 29  waist_mag_z        22403 non-null  float64\n",
      " 30  ankle_acc_x        22403 non-null  float64\n",
      " 31  ankle_acc_y        22403 non-null  float64\n",
      " 32  ankle_acc_z        22403 non-null  float64\n",
      " 33  ankle_gyro_x       22403 non-null  float64\n",
      " 34  ankle_gyro_y       22403 non-null  float64\n",
      " 35  ankle_gyro_z       22403 non-null  float64\n",
      " 36  ankle_mag_x        22403 non-null  float64\n",
      " 37  ankle_mag_y        22403 non-null  float64\n",
      " 38  ankle_mag_z        22403 non-null  float64\n",
      " 39  subject            22403 non-null  int64  \n",
      " 40  activity           22403 non-null  int64  \n",
      " 41  trial              22403 non-null  int64  \n",
      "dtypes: float64(39), int64(3)\n",
      "memory usage: 7.3 MB\n"
     ]
    }
   ],
   "source": [
    "features = features.dropna()\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f23b6fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.324326Z",
     "start_time": "2025-02-15T17:56:43.245939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rightpocket_acc_x', 'rightpocket_acc_y', 'rightpocket_acc_z',\n",
       "       'chest_acc_x', 'chest_acc_y', 'chest_acc_z', 'chest_gyro_x',\n",
       "       'chest_gyro_y', 'chest_gyro_z', 'chest_mag_x', 'chest_mag_y',\n",
       "       'chest_mag_z', 'wrist_acc_x', 'wrist_acc_y', 'wrist_acc_z',\n",
       "       'wrist_gyro_x', 'wrist_gyro_y', 'wrist_gyro_z', 'wrist_mag_x',\n",
       "       'wrist_mag_y', 'wrist_mag_z', 'waist_acc_x', 'waist_acc_y',\n",
       "       'waist_acc_z', 'waist_gyro_x', 'waist_gyro_y', 'waist_gyro_z',\n",
       "       'waist_mag_x', 'waist_mag_y', 'waist_mag_z', 'ankle_acc_x',\n",
       "       'ankle_acc_y', 'ankle_acc_z', 'ankle_gyro_x', 'ankle_gyro_y',\n",
       "       'ankle_gyro_z', 'ankle_mag_x', 'ankle_mag_y', 'ankle_mag_z',\n",
       "       'activity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_columns = [col for col in features.columns if any(x in col for x in ['acc', 'gyro', 'mag'])]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = features.copy()\n",
    "features_scaled[sensor_columns] = scaler.fit_transform(features[sensor_columns])\n",
    "\n",
    "features_scaled = features_scaled[sensor_columns + ['activity']]\n",
    "features_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85f7e718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.330141Z",
     "start_time": "2025-02-15T17:56:43.326528Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_sequences(sequences, n_steps, stride):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(sequences) - n_steps + 1, stride):\n",
    "        end_ix = i + n_steps\n",
    "        \n",
    "        seq_x = sequences.iloc[i:end_ix].iloc[:, :-1]  \n",
    "        seq_y = sequences.iloc[end_ix-1].iloc[-1]      \n",
    "        \n",
    "        X.append(seq_x.values)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a52bff2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:43.377020Z",
     "start_time": "2025-02-15T17:56:43.332731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADL1 (train) shape: (16486, 40)\n",
      "ADL2 (test) shape: (16487, 40)\n",
      "Fall1 (test) shape: (1436, 40)\n",
      "Fall2 (test) shape: (1436, 40)\n"
     ]
    }
   ],
   "source": [
    "adl_count = features_scaled['activity'].value_counts()[0]\n",
    "fall_count = features_scaled['activity'].value_counts()[1]\n",
    "\n",
    "adl_mask = features_scaled['activity'] == 0\n",
    "adl_data = features_scaled[adl_mask]\n",
    "adl1 = adl_data.iloc[:int(adl_count * 0.8)]    # 80% for training\n",
    "adl2 = adl_data.iloc[int(adl_count * 0.2):]    # 20% for testing\n",
    "\n",
    "fall_mask = features_scaled['activity'] == 1\n",
    "fall_data = features_scaled[fall_mask]\n",
    "fall1 = fall_data.iloc[:int(fall_count * 0.8)]  # 80% for testing\n",
    "fall2 = fall_data.iloc[int(fall_count * 0.2):]  # 20% for testing\n",
    "\n",
    "print(f\"ADL1 (train) shape: {adl1.shape}\")\n",
    "print(f\"ADL2 (test) shape: {adl2.shape}\")\n",
    "print(f\"Fall1 (test) shape: {fall1.shape}\")\n",
    "print(f\"Fall2 (test) shape: {fall2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "310faf23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.102712Z",
     "start_time": "2025-02-15T17:56:43.380265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (823, 40, 39)\n",
      "Train labels: (823,)\n",
      "Test data: (963, 40, 39)\n",
      "Test labels: (963,)\n"
     ]
    }
   ],
   "source": [
    "WINDOW_LENGTH = 40\n",
    "STRIDE = 20\n",
    "\n",
    "train_data, train_labels = split_sequences(adl1, WINDOW_LENGTH, STRIDE)\n",
    "\n",
    "test_adl_data, test_adl_labels = split_sequences(adl2, WINDOW_LENGTH, STRIDE)\n",
    "test_fall1_data, test_fall1_labels = split_sequences(fall1, WINDOW_LENGTH, STRIDE)\n",
    "test_fall2_data, test_fall2_labels = split_sequences(fall2, WINDOW_LENGTH, STRIDE)\n",
    "\n",
    "test_data = np.concatenate([test_adl_data, test_fall1_data, test_fall2_data])\n",
    "test_labels = np.concatenate([test_adl_labels, test_fall1_labels, test_fall2_labels])\n",
    "\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Train labels: {train_labels.shape}\")\n",
    "print(f\"Test data: {test_data.shape}\")\n",
    "print(f\"Test labels: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7fb1ac",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75320296",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.108700Z",
     "start_time": "2025-02-15T17:56:45.105171Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "INITIAL_LEARNING_RATE = 1e-3  \n",
    "DECAY_STEPS = 1000  \n",
    "DECAY_RATE = 0.98  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "623e5f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.116019Z",
     "start_time": "2025-02-15T17:56:45.111300Z"
    }
   },
   "outputs": [],
   "source": [
    "class UMAFallDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Transpose data from [batch, window_length, features] to [batch, features, window_length]\n",
    "        data = data.transpose(0, 2, 1)\n",
    "        self.data = torch.FloatTensor(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]  # Returns shape [features, window_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa953720",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.125309Z",
     "start_time": "2025-02-15T17:56:45.119105Z"
    }
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding='same'),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d90fd9bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.135765Z",
     "start_time": "2025-02-15T17:56:45.129393Z"
    }
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dd971a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.158063Z",
     "start_time": "2025-02-15T17:56:45.140267Z"
    }
   },
   "outputs": [],
   "source": [
    "class UNet1D(nn.Module):\n",
    "    def __init__(self, in_channels=39):\n",
    "        super(UNet1D, self).__init__()\n",
    "        \n",
    "        # Sinusoidal time embeddings\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(32),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool1d(2, padding=0)\n",
    "        \n",
    "        self.enc2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool1d(2, padding=0)\n",
    "        \n",
    "        self.enc3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool1d(2, padding=0)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(256, 512)\n",
    "        \n",
    "        # Time feature projection\n",
    "        self.time_mlp = nn.Linear(64, 512)\n",
    "        \n",
    "        # Decoder with output padding to match encoder sizes\n",
    "        self.upconv3 = nn.ConvTranspose1d(512, 256, 2, stride=2, output_padding=0)\n",
    "        self.dec3 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose1d(256, 128, 2, stride=2, output_padding=0)\n",
    "        self.dec2 = DoubleConv(256, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose1d(128, 64, 2, stride=2, output_padding=0)\n",
    "        self.dec1 = DoubleConv(128, 64)\n",
    "        \n",
    "        self.final_conv = nn.Conv1d(64, in_channels, 1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "        \n",
    "        # Encoder\n",
    "        e1 = self.enc1(x)\n",
    "        p1 = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)\n",
    "        p3 = self.pool3(e3)\n",
    "        \n",
    "        # Bottleneck with time embedding\n",
    "        b = self.bottleneck(p3)\n",
    "        b = b + t_emb.unsqueeze(-1)\n",
    "        \n",
    "        # Decoder with size adjustment if needed\n",
    "        d3 = self.upconv3(b)\n",
    "        if d3.shape[-1] != e3.shape[-1]:\n",
    "            d3 = F.interpolate(d3, size=e3.shape[-1], mode='linear')\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.upconv2(d3)\n",
    "        if d2.shape[-1] != e2.shape[-1]:\n",
    "            d2 = F.interpolate(d2, size=e2.shape[-1], mode='linear')\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.upconv1(d2)\n",
    "        if d1.shape[-1] != e1.shape[-1]:\n",
    "            d1 = F.interpolate(d1, size=e1.shape[-1], mode='linear')\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        return self.final_conv(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5d9911b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.175017Z",
     "start_time": "2025-02-15T17:56:45.161809Z"
    }
   },
   "outputs": [],
   "source": [
    "class DDPM:\n",
    "    def __init__(self, num_timesteps=100, beta_start=1e-4, beta_end=0.02):\n",
    "        \"\"\"\n",
    "        num_timesteps: total diffusion steps T\n",
    "        beta_start, beta_end: β₁ and βₜ for linear schedule\n",
    "        \"\"\"\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # Linear variance schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "    def q_sample(self, x_0, t):\n",
    "        \"\"\"\n",
    "        Forward diffusion process: q(x_t | x_0)\n",
    "        Samples from q(x_t | x_0) = N(x_t; √(αₜ)x₀, (1-αₜ)I)\n",
    "        \"\"\"\n",
    "        sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod[t])[:, None, None]\n",
    "        sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod[t])[:, None, None]\n",
    "        epsilon = torch.randn_like(x_0)\n",
    "        return sqrt_alphas_cumprod * x_0 + sqrt_one_minus_alphas_cumprod * epsilon, epsilon\n",
    "    \n",
    "    def sample_t(self, batch_size):\n",
    "        \"\"\"\n",
    "        Uniformly sample timesteps t\n",
    "        \"\"\"\n",
    "        return torch.randint(0, self.num_timesteps, (batch_size,))\n",
    "        \n",
    "    def p_sample(self, model, batch_size, device, shape):\n",
    "        \"\"\"\n",
    "        Reverse diffusion process: p_θ(x_{t-1} | x_t)\n",
    "        Generate samples using the reverse process\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_t = torch.randn(batch_size, *shape).to(device)\n",
    "            \n",
    "            for t in reversed(range(1, self.num_timesteps)):\n",
    "                time_tensor = torch.ones(batch_size, dtype=torch.long).to(device) * t\n",
    "                \n",
    "                # Predict noise\n",
    "                predicted_noise = model(x_t, time_tensor)\n",
    "                \n",
    "                alpha = self.alphas[t][:, None, None]\n",
    "                alpha_cumprod = self.alphas_cumprod[t][:, None, None]\n",
    "                beta = self.betas[t][:, None, None]\n",
    "                \n",
    "                # Add noise for stochasticity\n",
    "                noise = torch.randn_like(x_t)\n",
    "                \n",
    "                # Compute mean for p_θ(x_{t-1} | x_t)\n",
    "                x_t = 1 / torch.sqrt(alpha) * (\n",
    "                    x_t - (1 - alpha) / torch.sqrt(1 - alpha_cumprod) * predicted_noise\n",
    "                ) + torch.sqrt(beta) * noise\n",
    "                \n",
    "        model.train()\n",
    "        return x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb8d98da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:56:45.185735Z",
     "start_time": "2025-02-15T17:56:45.178296Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_ddpm(model, train_loader, ddpm, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        n_batches = len(train_loader)\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            # batch is already [batch, features, window_length]\n",
    "            x_0 = batch.to(device)\n",
    "            batch_size = x_0.shape[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            t = ddpm.sample_t(batch_size).to(device)\n",
    "            x_t, epsilon = ddpm.q_sample(x_0, t)\n",
    "            epsilon_theta = model(x_t, t)\n",
    "            loss = F.mse_loss(epsilon_theta, epsilon)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / n_batches\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} - Avg Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9ba7e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (823, 40, 39)\n",
      "Dataset sample shape: torch.Size([39, 40])\n",
      "Batch shape: torch.Size([32, 39, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  9.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 - Avg Loss: 1.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/300 - Avg Loss: 1.0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/300 - Avg Loss: 0.9942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/300 - Avg Loss: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/300 - Avg Loss: 0.9735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  8.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/300 - Avg Loss: 0.9557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/300 - Avg Loss: 0.9407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/300: 100%|██████████████████████████████| 26/26 [00:03<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/300 - Avg Loss: 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/300: 100%|██████████████████████████████| 26/26 [00:02<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/300 - Avg Loss: 0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/300: 100%|█████████████████████████████| 26/26 [00:03<00:00,  8.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/300 - Avg Loss: 0.8951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/300: 100%|█████████████████████████████| 26/26 [00:03<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/300 - Avg Loss: 0.8803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/300: 100%|█████████████████████████████| 26/26 [00:03<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/300 - Avg Loss: 0.8740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/300: 100%|█████████████████████████████| 26/26 [00:03<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/300 - Avg Loss: 0.8641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/300: 100%|█████████████████████████████| 26/26 [00:03<00:00,  8.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/300 - Avg Loss: 0.8541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/300:  62%|█████████████████▊           | 16/26 [00:01<00:01,  8.30it/s]"
     ]
    }
   ],
   "source": [
    "model = UNet1D().to(device)\n",
    "ddpm = DDPM(num_timesteps=100)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)  \n",
    "\n",
    "print(\"Train data shape:\", train_data.shape)\n",
    "\n",
    "train_dataset = UMAFallDataset(train_data)\n",
    "sample_data = train_dataset[0]\n",
    "print(\"Dataset sample shape:\", sample_data.shape)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "first_batch = next(iter(train_loader))\n",
    "print(\"Batch shape:\", first_batch.shape)\n",
    "\n",
    "train_ddpm(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    ddpm=ddpm,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=EPOCHS,\n",
    "    device=device    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33f079",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f53294",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.499Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_anomalies(model, ddpm, data_loader, device, threshold=None):\n",
    "    model.eval()\n",
    "    anomaly_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            x_0 = batch.to(device)\n",
    "            batch_size = x_0.shape[0]\n",
    "\n",
    "            t = ddpm.sample_t(batch_size).to(device)\n",
    "\n",
    "            x_t, epsilon = ddpm.q_sample(x_0, t)\n",
    "\n",
    "            epsilon_theta = model(x_t, t)\n",
    "\n",
    "            # MSE Loss (Anomaly Score)\n",
    "            loss = F.mse_loss(epsilon_theta, epsilon, reduction='none')\n",
    "            loss = loss.mean(dim=(1, 2)) \n",
    "            anomaly_scores.extend(loss.cpu().numpy())  \n",
    "\n",
    "    if threshold is None:\n",
    "        threshold = np.percentile(anomaly_scores, 95)\n",
    "\n",
    "    anomalies = [score > threshold for score in anomaly_scores]\n",
    "\n",
    "    return anomalies, anomaly_scores, threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f352d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.500Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_dataset = UMAFallDataset(test_adl_data)\n",
    "normal_loader = DataLoader(normal_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "abnormal_dataset = UMAFallDataset(np.concatenate([test_fall1_data, test_fall2_data]))\n",
    "abnormal_loader = DataLoader(abnormal_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f51252",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.501Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = UMAFallDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_anomalies, train_scores, _ = detect_anomalies(model, ddpm, train_loader, device)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.title('Loss Distribution of TRAIN ADL Data (NORMAL)', fontsize=16)\n",
    "sns.histplot(train_scores, bins=20, kde=True, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220970b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.502Z"
    }
   },
   "outputs": [],
   "source": [
    "normal_anomalies, normal_scores, _ = detect_anomalies(model, ddpm, normal_loader, device)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.title('Loss Distribution of TEST ADL Data (NORMAL)', fontsize=16)\n",
    "sns.histplot(normal_scores, bins=20, kde=True, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15581e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.504Z"
    }
   },
   "outputs": [],
   "source": [
    "abnormal_anomalies, abnormal_scores, _ = detect_anomalies(model, ddpm, abnormal_loader, device)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.title('Loss Distribution of TEST FALL Data (ABNORMAL)', fontsize=16)\n",
    "sns.histplot(abnormal_scores, bins=20, kde=True, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d0056",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.505Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.title('MSE Distribution Comparison')\n",
    "plt.hist(normal_scores, bins=50, alpha=0.5, color='blue', label='Normal')\n",
    "plt.hist(abnormal_scores, bins=50, alpha=0.5, color='red', label='Abnormal')\n",
    "plt.xlabel('Anomaly Score (MSE)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449ec1c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.506Z"
    }
   },
   "outputs": [],
   "source": [
    "error_df = pd.DataFrame({\n",
    "    'reconstruction_error': np.concatenate([normal_scores, abnormal_scores]),\n",
    "    'true_class': np.concatenate([np.zeros(len(normal_scores)), np.ones(len(abnormal_scores))])\n",
    "})\n",
    "\n",
    "print (\"error dataframe head : \")\n",
    "print(error_df.head(10))\n",
    "print ()\n",
    "print (\"error dataframe tail : \")\n",
    "print(error_df.tail(10))\n",
    "print ()\n",
    "print(error_df.describe())\n",
    "print ()\n",
    "error_df['true_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b238bf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.507Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='true_class', y='reconstruction_error', data=error_df, showfliers=False)\n",
    "plt.ylabel('Reconstruction Error Distribution')\n",
    "plt.xlabel('Class (0: Normal, 1: Abnormal)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34387f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.508Z"
    }
   },
   "outputs": [],
   "source": [
    "# threshold = np.percentile(normal_scores, 90)\n",
    "thresshold = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d580ef8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.510Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = error_df.groupby('true_class')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.reconstruction_error, marker='^', ms=3.5, linestyle='',\n",
    "            label=\"Normal\" if name == 0 else \"Fall\")\n",
    "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction Error by Class\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.xlabel(\"Data Point Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f22a54",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.511Z"
    }
   },
   "outputs": [],
   "source": [
    "LABELS = [\"Normal\", \"Abnormal\"]\n",
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel('True Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd60c0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-15T17:56:31.512Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(error_df.true_class, y_pred, average='binary')\n",
    "accuracy = accuracy_score(error_df.true_class, y_pred)\n",
    "\n",
    "print('\\nPerformance Metrics:')\n",
    "print(f'Accuracy Score : {accuracy:.4f}')\n",
    "print(f'Precision     : {precision:.4f}')\n",
    "print(f'Recall        : {recall:.4f}')\n",
    "print(f'F1 Score      : {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
